Tato dokumentace by mněla obsahovat vše potebné k mim pytoch projektům
https://www.learnpytorch.io/00_pytorch_fundamentals/

Neuronová síť
    Neuronová síť je program který spracovává číselná data a hledá v nich souvisloti.
    Nazákladě techto souvyslotí pracuje s daty a vyprodukuje výsledek který lze 
    převézt do námi požadovaného formátu (slova, pravděpodobnost, odhad, překlad...)
    Neurovoá síť je složená z neuronů.
    Neurony jsou naskládány do tří vrstev :
    Vstupová => přijímá a převádí data na čísla 
    Skrytá => provádí matematické operace na datech, hledá souvyslosti ... upravuje data
    tak aby jsme získali náš výsledek
    výstupní => překládá a u ukazuje data ze skryté vrstvy ve formně námy požadovaného 
    výsledku.

Neuronová síť V2
    Neuronové sí tě jsou velké konbinace linearní ch a nelinearních funkcí
    Které jsou potencialné schopné hledat souvyslosti v datech

Zkráceně fungování neuronové sítě
    Začínají s náhodnýmy daty -> operace s tensory -> uprava dat tak aby lépe 
    představovaly požadovaný výsledk -> celé znovu -> celé znovu....

Typy učení neuronových sítí 
    Supervised learnig / Kontrolované učení
        Využívá se v případě že máme data a popiss dat -> recept v kuchařce.
        Nebo fotka a popys to ho ce je na fotce -> Fotka1 ... pes
    Unsupervised learnig & Self-supervised learning / Nekontrolované učení & Samokontrolované učení
        Používá se v případě že neexistuhe popis dat.
        Naučí so poznavat rozdily mezi jednotlivými souvyslosti a zařadí souvysloti
        => Fotka1 ... není to pes je to kočka protože má delší ocas ... má delší ocas tak je to kočka
    Transfer learning / učení zdílením
        Neuronové sítě mezi sebou zdílí na co přišli a tím si navzájem pomáhaji
        Např. skonbinujeme jednu síť která funguje ja na základě Kontrolované učení a 
        druhou která využívá Nekontrolované učení & Samokontrolované učení.

Co je potřeba k učení neuronové sítě
    Loss funkce -> zjištuje jak špatný je výsledek neuronové sítě
    Optimazer -> za pomocí loss funkce upravuje parametry za cíle získat lepší výsledek.
                Lepší výsledek = nižší hodnota loss funkce
    Dále specificky pro PyTorch:
        1. Trenigová smíčka/loop
        2. Testová smička/loop

Využití neuronových sítí & deep learnig 
    Dají se využí na vše pokud se povede převést vstupní data na čísla.
    V ten moment vnich neuronová síť vždi může najít nějáké souvyslosti a eventuelně 
    mužete získat nějáké výsledky.
    Určité využití : 
        Překlad, doporučení, computer vision 

Nejčastjší errory v PyToch a v deeplearnig
    Tensors not right datatype
        Navzájem se soubou můžou pracovat jen tensory se stejným datatypem
    Tensors not right shape
        Špatná velikost tenzoru -> tenzory nejsou stejně velké
    Tensor not on the right device
        Tensory nejsou uloženy na stejném místě. Místa -> CPU, GPU, CUDA

Tensors / listy
    Jedná se o číselné hodnoty v různých formátech.
    V této knihovně se tensory vytváří pomocí příkazu -> torch.tensor()
    Existují 3 základní typy :

    Scalar / Skalár
        Bez žádných dimenzí čistě jedno číslo.
            Bod
        Je potřeba použít metodu .item() abych získali scalar jako normální číslov Pythonu

    Vector / 1D lyst / Vector
        Má jednu dimenzi -> jakoby jeden směr.
            Čára
        Klasický lyst v Pythonu.
        Slkádá se z vícce čísel v hrantých závorkách.
        Např. [1, 2, 3]
    
    Matrix / 2D list
    Má dvě dimenze -> dva různé směry
        čtverec

    Tensor / 3D list
    Má tři dimenze -> má tři různé směry
        Krychle

Datatypy Tensorů
    Jednotlivé tensory můžou mít různé data typy (bolean, float16, float32, ...)
    Jedná se to s jakou přesností se ukládají data tensoru
    float32 = jedno číslo bude zabírat 32 bitu v pamněti.

Získávání dat z tensoru
    Pro získání datatypu ->         tensor.dtype
    Pro získání shape / tvar ->     tensor.shape
    Pro získání device ->           tensor.device

Manipuleace s tensory
    Existuje základních pět operací u které lze provádět při vytváření euronových sítí:
        Přidání                     Addition
        Odčítání (odebírání)        Subtraction
        Násobení - elemetary-wise   Mutipication - elemetary-wise 
        Dělení                      Division
        Násobení matrixů            Matrix multiplication

Násobení matrixu -> https://www.mathsisfun.com/algebra/matrix-multiplying.html
    Pro lepší představu: http://matrixmultiplication.xyz/
    @ - značka pro nasobeni matrixu
    Jedná so o sčítání dvou matryxů (2D listů).
    Existují 2 pravidla pro násobení matrixu, která když se edodrží tak dojde k erroru
        1. Tensory musí mít stejný tvar 
            (X, Y) - X = počet listů (počet sloupců)
                   - Y = počet hodnot (délka řádku)
            (X, Y) @ (X, Y) -> nebude fungovat
            (Y, X) @ (X, Y) -> bude fungovat
            Protože platí že Y v jednom listu se musí ronat Y v listu druhém
            to stejné platí pro X
            Je to spůsobeno tím že při násobení matrixů násobíme řádek 1. listu se sloupcem listu 2.
            Tedy pokud nemáme stejně dlouhé řádky i sloupce hodnoty se nebudou mít s čím násobit.
        2. Výsledný matrix bude mít tvar X z matrixu 1. a Y z matrixu 2.
            Tedy:
            (3, 2) @ (2, 3) = (3, 3)
            Porože platí že tvar se bude rovnat počet lisů z 1. matrixu a popočet hodnot z 2, matrixu
            prože zase platí že násobíme řádky z 1. matrixu se sloupci z 2. matrixu
        To lze řešit pomocí Transpose:
            Transpose
                Zmnění v metrixu řádky na sloupce -> X -> Y a Y -> X
                Volání: .T
    Tímto způsobem :
        Pro získání první hodnoty se musí vynásobyt první hodnota v prvním
        řádku listu v jednom metrixu s první hodnotou prvního sloupce
        v listu druhém. A stejným spusobem to pokračuje na druhé hodnoty a třetí a ...
        Takto zíáskané hodnoty se navtájem sečtou a výsledným součtem těch to hodnot je 
        první pole v novém metrixu.
    Sčítání pokračuje tímto způsobem:
        Nejprve se musí sečíts první řádek se všemi sloupci v metrixu.
        Nasledně drůhý řádek se všemi sloupci pak třetí ...

Úpravy tensoru  
    Reshaping
        Zmnění tvar tensoru podle definovaného tvaru
        .reshape(První hodnota = počet řádků, Druhá hodnota = počet sloupců)
    View
        Zobrazí tenzor požadovaným spůsobem.
        Veiw hodnoty zdílí pamněť originalu -> zmněněním vei se zmnění i original
    Stacking
        Spojí tensory dohromady buď podle sloupců (vstack) nebo podle řádku (hstack)
        Např. A = [1, 2]         
              B = [3, 4]
              torch.vsack((A, B)) = tensor([[1],
                                            [2],
                                            [3],
                                            [4]])
              torch.hsack((A, B)) = tensor([[1], [2], [3], [4]])
    Squeeze 
        Odstraní všechny dimenze o velikosti 1 z tensoru (odstraní hranaté zavorky)
        Udělá z matrixu [1, 9] -> [9]
                        [1, 9, 3, 1, 1] -> [9, 3]
    Unsqueeze
        Přidá dimenzi o velikosti 1 do tensoru
    Permute 
        Vrátí view vstupní hodnoty se specificky pěmněněným pořadím dimenzí,
    
Tvar / Shape
    Tvar určuje jak velký je list a kolik listů je v listu.     
    Prozor záleží na typu listu.
    List = [1, 2, 3]        Shape: (3)          1D, List        List ==     Array ==        Vektor
    List = [[1, 2, 3],      Shape: (2, 3)       2D, Matrix      Python      Programování    Matematika
            [1, 2, 3]]
    List = [[[1, 2, 3],    Shape: (3, 2, 3) = tři listy po dvou listech s třemi hodnotami
                [1, 2, 3]],                     3D, Array
            [[1, 2, 3],
                [1, 2, 3]],
            [[1, 2, 3],
                [1, 2, 3]]]

torch.tensor(X)         X, Y = zvolená hodnota
    Vytvoří tenzor na základě X.
    Pro každý typ tenzoru je nutné zdat X jinak :

    Scalar -> torch.tensor(X) 
    Vector -> torch.tensor([X, Y]) 
    V případě že se jedná o skalar je potřeba použít metodu .item()

torch.rand(X)           X = zvolená hodnota
    Vytvoří tenzor s tvarem X.
    Tento tensor v sobě budemít náhodné hodnoty.

torch.zeros(X)          X = zvolená hodnota
    Vytvoří tenzor o velikosti X pouze s nulami.

torch.arange(X)         X = zvolená hodnota
    Vytvoří řadu čísel. 
    Lze aplikovat tůzné parametry -> torch.arange(start=0, end=1000, step=77)

torch.matmul(tensor1, tensor2) = torch.mm(tensor1, tensor2)  # je to stejné
    Vynásobí mezi sebou zadané tenzory (v tomto případě tensor1 a tensor2)

model.state_dict()              model = model / neuronová síť
    Vypíše jednotlivé parametry modelu i s jejich názvy.
Transpose
    Zmnění v metrixu řádky na sloupce -> X -> Y a Y -> X
    Volání: .T

Agregace
    Agregace =  hledání: sumi, min, max, mean...
    mean = průměr
    argmin(tensor) - najde idenx minimalni hodnoty v tensoru
    argmax(tensor) - najde idex maximalni hodnoty v tesnoru

Převá dění tenzoru z a do numpy
    Pro převedení numpy array na pytorch tenzor se používa metoda torch.from_numpy()
    Metoda from_numpy převede původní array tedy => pokud array upravíme sejně dostaneme puvodni verzi (ta kterou jsem vygenerovali)
    To stejné platí i pro převrácenou metodu torch.Tensor.numpy()
    Pro převrácen proces se používá torch.Tensor.numpy()
    Numpy ma defaultně formát float64 -> Long
    PyTorch ma defaultně formát float32 -> None

Reprodukce 
    Cílem reprodukce je vytvořit náhodná data která se ale pokaždé vyvoří stejná pokud jsou stejné i podmínky
    Pro reprodukci je dobré si vyvořit konstantu RANDOM_SEED = X (X = nahodné číslo, často 42)
    Poté se zavolá funkce torch.manual_seed(RANDOM_SEED) 
    Atímto se nastavý že pokud se vygenerují tenzory s náhodnýmy čísli o stejném tvaru tak budou mít identické hodnoty
    Tedy: 
        torch.manual_seed(RANDOM_SEED)
        random_tensor_C = torch.randn(3, 4)
        torch.manual_seed(RANDOM_SEED)
        random_tensor_D = torch.randn(3, 4) 
    v tomto případě bue platit že : random_tensor_C = random_tensor_D

Spouštění PyTorch programů na GPU
    Velmi se vyplatí spouště programi na GPU prože matematické operace budou probíhat výrazně rychleji.
    POZOR -> Numpy fnguje pouze na CPU 

Přesouvání dat mezi CPU a GPU
    Pro převedení na GPU -> tensor.to(device)      # device = "cuda"
    Pro převedení na CPU -> tensor.cpu()

Workflow / Pracovní popis
    Obsah:
        1. Příprava dat 
        2. Stavba modelu
        3. Učení modelu na připravených datech datech
        4. Dělání předpovědí a hodnocení modelu
        5. Ukládání a vkládání modelu
        6. Kombinování všeho předešlého dohromady
    from torch import nn -> vše co PyTorch nabýzí pro neuronové sítě.

                                        MODEL  =  NEURONOVÁ SÍŤ
tahák -> https://pytorch.org/tutorials/beginner/ptcheat.html
Forward metoda / def forward()
    Jedná se o součást většiny class, funkcí a modelů.
    Forward je ta část která realě dělá výpočet výsledku dané claass nebo modelu.
    Všechny podkategorie nn.Module (nn.Parametr...) požadují po svém připsání přepsání forward metody.
    přiklad forward metody :
        def foorward(self, x: torch.Tensor) -> torch.Tensor:    vysvětlení:
        říká že x má format torch tensor a ze funkce vrátí torch tensor

from torch import nn
    Obsahuje všechny před nastavené funkce a modele potřebné ke konstrukci neuronové sítě 
    které může pytoch nabýdnout.

torch.nn.Parametr
    Které parametry by se síť mněla naučit a vyskoušet
    => hodnoty které za nás síť sama dosadí a by dosáhla požadovaného výsledku.

torch.nn.module
    Základni class pro všechny moduly (neuronové sítě)
    Pokud zní necháváme dědit date (subclass) je nutné přepsat forward metodu

nn.Linear()
    Vytvoří vrstvu pro linearni model.
    K tomu potřebuje mit nadefinovány parametry 
    nn.Linear(in_features=1,    # kolik hodnotot dostane vyrstva -> 1 X_train
            out_features=1)   # kolik hodnotot vyplinve vyrstva -> 1 y_train
    Využívá pro vypočet rovnici -> Y = weight * x + bias

torch.optim
    Místo kde jsou uloženy optimazeri daného modulu.
    (např. pomáhá s gradial desent)

with torch.inference_mode():    inference = predikce
    Určí že výsledky této "funkce".
    Odpojí všechny kontroly a zbýraní dat které se dělá při učení sítě na jejích výsledcích.
    Výsledk? -> rychlejší získání výsledků (predikcí)
Data
    Data můžou bít skoro vše např.: video, fotka, text...
    Deeplearnig se skládá z vou hlavních kroků:
        Převedení dat na čísla.
        Vytvoření modelu aby mezi těmito čísli našel spojitosti
    Jeden z nejdůležitějších konceptů v práci s daty je(generalizace):
        Rozděleni dat na trénigové ,ohodnocené a testové data:
        Trénigové data / training st/split 60-80% všech dat
            Data na kterýchs e model učí.
        Ohodnocená data / validation set 10-20% všech dat    (není nutné mít tyto data ale je to občas lepší.)
            Zde hodnotíme jak dobře model data zpracoval.
        Testová data / test set/split    10-20% všech dat
            Z těcto zpracovaných dat nám model dá finální výsledek.     Předvede svou generalizaci
    Generalizace
        Schopnost modelu dobře spracovat data která jesše neviděl

Linear regression
    Vzorec: Y = f(X,B) + e
    je matematická metoda používaná pro proložení souboru bodů v grafu přímkou.

Parametr
    Je součást rovnice/vzorce se se modelu učí.
    Např.: weight, bias....

Cost funkce = Loss funkce = criterion
    Funkce ketá porovnává výsledek neouronové síť s požadovaným / správným  výsledkem.
    Toho dosahuje odečtem požadovaných hodnot od hodnout výsledných sítě a následným umocněním výsledku na 2
    => určuje jak špetné jsou výsledky neuronové sítě (čím menší číslo tím lepší)

Optimazer
    Optimazer využí vá loss funkce a na jejím základě upravuje model tak aby co nejefektivnějí
    snížil hodnotu loss funkce.
    Jako optimizeri se používa spousta algoritmů 
    V PyToch je najdeme pod torch.optim -> https://pytorch.org/docs/stable/optim.html

Gradiant desent
    Jedná se o algoritmus za cílem najít lokální minimum funkce.
    Tedy hledá nejnižší možný bod na grafu funkce.
    Tohto leze dosáhnout pomocí takzvané cost funkce která určuje chybnost výpočtu neruronové sítě
    Zároveň tento algoritmus využívá derivce ((takové té tenčny na grafu ;) )

Back propagation
    Jedná se o algoritmus který pomáhá učit neuronovou síť a upřesňuje její výsledky.
    Tohoto dosahuje pomocí určování duležitosti -> míra vlivu na cost funkci jednotlyvých parametrů.

lr=X        X -> zvolená hodnota (defaultnš je 0.1)
    Jedná se o learnig rate.
    Určuje jak moc se zmnění parametry během jednoho opakování

prams=model.Parametr()                  model = neuronová síť
    Určí optimizeru které parametry má upravit

Hyperparamet
    Parametr nastavený člověkem.

training loop
    Jak funguje:
    0. Proskenování dat
    1. Forward pass (prohnání dat skrz metodu forward()) - také nazívané jako forwad propagation
        za cílem vytvořit nějákou před předpovědí
    2. Zpočítání loss funkce - (srovnání výsledku modelu s správnýmy výsledky)
    3. Optimazer zero grad  nastavý hodnotu optimazeru na 0 aby se nezpomaloval výpočet (jak? nevím)
    4. Loss backkwards -> pohybuje se od výsledku aby zjistíl spád jednolivých parametrů
        => (spád -> míra významnosti pro výsledek)  - (Back propagation)
    5. Optimazer step - Využití optimizeru k upravě parametrů tak aby se snížila hodnota 
        z loss funkce. - (Gradiant desent)

epocha /epoch
    Je jedno opakování cyklu

model.train()       Model = neuronová sít
    Nastavý všechny parametry které potřebukí sklon aby potřebovaly sklon

Ukládání a nahrávání modelů v pytorch - https://pytorch.org/tutorials/beginner/saving_loading_models.html
    1. pomocí torch.save() - uložený předmnět v pytorch do formatů pickle od Pythonu
    2. torch.load() - umožní nahrát předmnět v pytoch
    3. torch.nn.Module.load_state_dict() - umožní uložit všechny parametry modelu
    Po uložení čistě parametru modelu je vhodné udělat nový model a nahrát parametry tam
    pth -> formát pro pythorch

Postup Ukládání + nahrávání (state_dict -> parametry)
    1. Je nutné importovat knihonu pathlib a následně zní pathlib
        import pathlib 
        from pathlib import Path
    2. Vytvoří se a pojmenuje místo kam se bude model ukládat (pomocí Path)
        MODEL_PATH = Path("models")
        MODEL_PATH.mkdir(parents=True, exist_ok=True)
    3. Pojmenování modelu
        MODEL_NAME = "___"
    4. Určí se místo uložení (pomocí cesty /path)
        MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME
    5. Samotné uložení
        torch.save(obj=model_0.state_dict(), f=MODEL_SAVE_PATH)
    6. Je vhodné parametry nahrát do nového modelu ale jinak :
        Loaded_model.load_state_dict(torch.load(f=MODEL_SAVE_PATH))

Data sety 
    Imagenet -> https://www.image-net.org/index.php

Klasifikace
    Je proces kdy musíme rzhodnout o kterou z možností se jedná / určit o co se jedná
    Pro klasifikaci je nutné reprezentvat data jako čísla (data = např. fotky)
    Binari classification
        Rozhodnutí mezi dvěma možnostmi.    -> je spam / není spam
    Multiclass classification
        Více než dvě možnosti pro Rozhodnutí.
        Každá class ma ale pouze jedno jmeno (popis /tag)
    Multilabel classification
        Více než dvě možnosti pro Rozhodnutí.
        Každá class ma více jmen (popisu /tagu)

Konstrukce klasifikačního modelu
    Struktura:
        Input layer - přijímá data
        Hidden layer('s) - zpracovává data
            Neurons per hidden layer - počet neuronů na zkrytou vrstvu
        Hidden layer activation function - zpusob zpracování dat
        Output layer - vydává výsledky a převádí ho do čitelné podoby
        Output activation - počítá výsledek
        Loss function - počítá nesprávnost výpočtu
        Optimizer - snižuje nesprávnost výpočetu

Toy Dataset
    Malí dataset na kterém se dá experimentovat ale dost velký
    nato aby se na něm daly trénovat základy

Jak vybrat loss funkci?
    Vždy záleží na specifickém problému
    Pro linearni regresi -> MAE nebo MSE (mean absolute error / mean squared error)
    Pro klacifikaci -> binary cross entropy or categorical cross entropy (cross entropy) 
    Pro připomenutí - loss funkce počítá nesprávnost výpočtu sítě

Jak vybrat optimizer ?
    Zase záleží na daném problému problému 
    Nejpoužívanější (pro kategorizaci) jsou SGD(Stochastic gradient descent) a Adam
    Každopádně PyTorch má spoustu vbudovaných optimizeru takze stací vybrat na základě potřeby


Binary cross entropy 
    https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a
    Jedná se o loss funkci které počítá loss za pomoci záporné sumi z logarytmu pravděpodobnosti.
    Zjednodušeně -> počítá loss pomocí logaritmů.
    Využívá se pro binární klasifikaci.

Categorical cross entropy
    Je funkce pro kterou vužíváme One-hot  a pravděpodobnost z Softmax.
    Jedná se o zápornou sumu z hodnot One-hot  kdy každou hodnotu než ji přičteme
    k jiné platí že ji vynásobýme přirozeným logaritmem se zkladem jako hodnota z 
    pravděpodobnosti se stejným indexem jako One-hot . 
    Př. X = výsledek 
        Class: 3 
        Label: 0
        One-hot:            [1, 0, 0]
        pravděpodobnost:    [0,7, 0.1, 0.2]
        X = -(1 * log(0,7) + 0 * log(0,1)+ 0 * log(0,2))
    V kosce, se jedná o kombinaci Softmax a Cross entropy využívanou pro muticlass kategorizaci

Softmax activation function
    Převádí čísla na pravděpodobnost. 
    Suma všech pravděpodobností je vždi 1.
    Softmax se používá proto aby jsme se vyvarovaly problému s zápornýmy hodnotami 
    ve výsledku.
    Tedy abychom mohly správně určit jak špatný byl výsledek.
    Proto použijeme exponencialní funkce se zakladem eulerova čísla. (e)
    Protože vždy bude rozdíl mezi e^2 a e^-2.
    Softmax je kombinace normalizace a exponecializace. 

Logit
    Výsledek neuronové sítě před předání dat do poslední vrstvy
    => hodnoty ze kterých síť vypočítá pravděpodobnost  pro daný výsledek.
    Tedy hodnoty které používápe pro Softmax a další podobné normalizace.(podbné funkce)

Sigmoid function 
    Sigmoidní funkce je každá matematická funkce, jejíž graf má charakteristickou křivku ve tvaru písmene S neboli sigmoidu.
    Běžným příkladem sigmoidní funkce je logistická funkce
    Je omezená, diferencovatelná, reálná funkce, která je definována pro všechny reálné vstupní hodnoty a má
    v každém bodě nezápornou derivaci a přesně jeden inflexní bod

Přechod z čistích logitu -> předpovězené šance -> předpovězené popisy
Going from raw lgits -> prediction prapabilitiess -> prediction labels 
    Výsledky našeho modelu budou čisté /raw logity
    Logity mužeme převést na prediction prapabilitiess (předpovězené šance) použitím 
     nějáké aktivační funkce (Sigmoid -> binarní klasifikace a softmax -> multiclass klasifikace)
    Následně můžeme prediction prapabilitiess(předpovězené šance) převést na prediction labels(předpovězené popisy)
     pomocí zaokrouhlení nebo argmax()

Způsoby vylepšení modelu
    Přidat výse vrstev - model má víc šancí nato se naučit data
    Zvíšit počet neouronů
    Zvětšit počet epoch - déle může zkoumat data
    Zmněnit aktivační funkci
    Upravit lernig rate
    Změnit loss funkci nebo optimizer nebo obojí

    Všechny tyto upravy jsou z hlediska modelu protože upravují pouze moedl a ne data
    protože vše věci výše zmíněné jsou věci které můžeme přepsat runě tak se nazívají hyperparametrs

Nelinearní model
    Linear = tovné čáry
    Nelinearní = křivky
    Obsahuje nelinearní a linearni funkce např. ReLu
    Ty mu dávají schopnost lépe hledat souvyslosti v datech -> lepší a rychlejší učení.

Computer vision (zkratka CV)
    Co řeší?
        Vpodstatě jaký koliv problék který lse vidět(zobrazit)
        Binary classification, Multiclass classification, Object detection, Segmentation
    Inputy a outputy
        Typcky se jedná o fotu nebo video které se převede na tensor s hodnotami pro výšku šířku a počet barev(3 - RGB)
        Shape toho tenzoru se často nazívý NHWC - number of batches, height, width, coulr channels   
Batches - dávky
    Jedná se o počet dat (dávku) kterou modelu poskytneme.
    -> nejčastěji se velikost je 32 (bitu, obrazků ...)
    Mini-batches    
        Jedná se o způsob spracování dat -> pčeřevedeme data na barche
        a následně je poskytneme síti stou zmněnou že provedeme optimalizaci
        po zpracování patche a né po konci epochy (opakování) -> model se
        může více naučit a probrem se výrazně zrychlí

CNN - convolutinonal neural network
    Konstrukace / stavba (běžná):
    Input layer                 Output layer
    convolutinonal layer        Hidden layer
                
Knihovný pro computer vision
    torchvision - hlavní a základní knihovna pro pytorch v případě CV
    torchvision.datasets - data sety a funkce pro nahrávání dat
    torchvision.models - předpřipraavené modely pro vlastní využití
    torchvision.transforms - pomáhá převádět fotky/ videa na čísla
    torchvision.utils data.Dataset - základní datasety
    torchvision.utils.data.DataLoader - pythonem opakovatelné datasety

PIL image
    Jedná se o fotku převedenou na tenzor ve tvaru Hight, Width, Color,
    s velikostí [0,255]

Trainig loop pro CV
    Trainig loop je o trochu jiný pro CV a pro kasifikaci
    Loop trough epochs - projít epochy
    Loop trough trainig batches, perform traing step, calculate rain loss 
     per batch - projít dávky, zoptimalizovat data, spočítat loss
    Loop trough testing batches, perform traing step, calculate rain loss 
     per batch - to stejné akorát pro testové dávky
    Vizualizae
    Zmněřit čas 

Tqdm
    knihovna pomáhající s vizualizací progersu sítě