Tato dokumentace by mněla obsahovat vše potebné k mim pytoch projektům
https://www.learnpytorch.io/00_pytorch_fundamentals/

Neuronová síť
    Neuronová síť je program který spracovává číselná data a hledá v nich souvisloti.
    Nazákladě techto souvyslotí pracuje s daty a vyprodukuje výsledek který lze 
    převézt do námi požadovaného formátu (slova, pravděpodobnost, odhad, překlad...)
    Neurovoá síť je složená z neuronů.
    Neurony jsou naskládány do tří vrstev :
    Vstupová => přijímá a převádí data na čísla 
    Skrytá => provádí matematické operace na datech, hledá souvyslosti ... upravuje data
    tak aby jsme získali náš výsledek
    výstupní => překládá a u ukazuje data ze skryté vrstvy ve formně námy požadovaného 
    výsledku.

Zkráceně fungování neuronové sítě
    Začínají s náhodnýmy daty -> operace s tensory -> uprava dat tak aby lépe 
    představovaly požadovaný výsledk -> celé znovu -> celé znovu....

Typy učení neuronových sítí 
    Supervised learnig / Kontrolované učení
        Využívá se v případě že máme data a popiss dat -> recept v kuchařce.
        Nebo fotka a popys to ho ce je na fotce -> Fotka1 ... pes
    Unsupervised learnig & Self-supervised learning / Nekontrolované učení & Samokontrolované učení
        Používá se v případě že neexistuhe popis dat.
        Naučí so poznavat rozdily mezi jednotlivými souvyslosti a zařadí souvysloti
        => Fotka1 ... není to pes je to kočka protože má delší ocas ... má delší ocas tak je to kočka
    Transfer learning / učení zdílením
        Neuronové sítě mezi sebou zdílí na co přišli a tím si navzájem pomáhaji
        Např. skonbinujeme jednu síť která funguje ja na základě Kontrolované učení a 
        druhou která využívá Nekontrolované učení & Samokontrolované učení.

Co je potřeba k učení neuronové sítě
    Loss funkce -> zjištuje jak špatný je výsledek neuronové sítě
    Optimazer -> za pomocí loss funkce upravuje parametry za cíle získat lepší výsledek.
                Lepší výsledek = nižší hodnota loss funkce
    Dále specificky pro PyTorch:
        1. Trenigová smíčka/loop
        2. Testová smička/loop

Využití neuronových sítí & deep learnig 
    Dají se využí na vše pokud se povede převést vstupní data na čísla.
    V ten moment vnich neuronová síť vždi může najít nějáké souvyslosti a eventuelně 
    mužete získat nějáké výsledky.
    Určité využití : 
        Překlad, doporučení, computer vision 

Nejčastjší errory v PyToch a v deeplearnig
    Tensors not right datatype
        Navzájem se soubou můžou pracovat jen tensory se stejným datatypem
    Tensors not right shape
        Špatná velikost tenzoru -> tenzory nejsou stejně velké
    Tensor not on the right device
        Tensory nejsou uloženy na stejném místě. Místa -> CPU, GPU, CUDA

Tensors / listy
    Jedná se o číselné hodnoty v různých formátech.
    V této knihovně se tensory vytváří pomocí příkazu -> torch.tensor()
    Existují 3 základní typy :

    Scalar / Skalár
        Bez žádných dimenzí čistě jedno číslo.
            Bod
        Je potřeba použít metodu .item() abych získali scalar jako normální číslov Pythonu

    Vector / 1D lyst / Vector
        Má jednu dimenzi -> jakoby jeden směr.
            Čára
        Klasický lyst v Pythonu.
        Slkádá se z vícce čísel v hrantých závorkách.
        Např. [1, 2, 3]
    
    Matrix / 2D list
    Má dvě dimenze -> dva různé směry
        čtverec

    Tensor / 3D list
    Má tři dimenze -> má tři různé směry
        Krychle

Datatypy Tensorů
    Jednotlivé tensory můžou mít různé data typy (bolean, float16, float32, ...)
    Jedná se to s jakou přesností se ukládají data tensoru
    float32 = jedno číslo bude zabírat 32 bitu v pamněti.

Získávání dat z tensoru
    Pro získání datatypu ->         tensor.dtype
    Pro získání shape / tvar ->     tensor.shape
    Pro získání device ->           tensor.device

Manipuleace s tensory
    Existuje základních pět operací u které lze provádět při vytváření euronových sítí:
        Přidání                     Addition
        Odčítání (odebírání)        Subtraction
        Násobení - elemetary-wise   Mutipication - elemetary-wise 
        Dělení                      Division
        Násobení matrixů            Matrix multiplication

Násobení matrixu -> https://www.mathsisfun.com/algebra/matrix-multiplying.html
    Pro lepší představu: http://matrixmultiplication.xyz/
    @ - značka pro nasobeni matrixu
    Jedná so o sčítání dvou matryxů (2D listů).
    Existují 2 pravidla pro násobení matrixu, která když se edodrží tak dojde k erroru
        1. Tensory musí mít stejný tvar 
            (X, Y) - X = počet listů (počet sloupců)
                   - Y = počet hodnot (délka řádku)
            (X, Y) @ (X, Y) -> nebude fungovat
            (Y, X) @ (X, Y) -> bude fungovat
            Protože platí že Y v jednom listu se musí ronat Y v listu druhém
            to stejné platí pro X
            Je to spůsobeno tím že při násobení matrixů násobíme řádek 1. listu se sloupcem listu 2.
            Tedy pokud nemáme stejně dlouhé řádky i sloupce hodnoty se nebudou mít s čím násobit.
        2. Výsledný matrix bude mít tvar X z matrixu 1. a Y z matrixu 2.
            Tedy:
            (3, 2) @ (2, 3) = (3, 3)
            Porože platí že tvar se bude rovnat počet lisů z 1. matrixu a popočet hodnot z 2, matrixu
            prože zase platí že násobíme řádky z 1. matrixu se sloupci z 2. matrixu
        To lze řešit pomocí Transpose:
            Transpose
                Zmnění v metrixu řádky na sloupce -> X -> Y a Y -> X
                Volání: .T
    Tímto způsobem :
        Pro získání první hodnoty se musí vynásobyt první hodnota v prvním
        řádku listu v jednom metrixu s první hodnotou prvního sloupce
        v listu druhém. A stejným spusobem to pokračuje na druhé hodnoty a třetí a ...
        Takto zíáskané hodnoty se navtájem sečtou a výsledným součtem těch to hodnot je 
        první pole v novém metrixu.
    Sčítání pokračuje tímto způsobem:
        Nejprve se musí sečíts první řádek se všemi sloupci v metrixu.
        Nasledně drůhý řádek se všemi sloupci pak třetí ...

Úpravy tensoru  
    Reshaping
        Zmnění tvar tensoru podle definovaného tvaru
        .reshape(První hodnota = počet řádků, Druhá hodnota = počet sloupců)
    View
        Zobrazí tenzor požadovaným spůsobem.
        Veiw hodnoty zdílí pamněť originalu -> zmněněním vei se zmnění i original
    Stacking
        Spojí tensory dohromady buď podle sloupců (vstack) nebo podle řádku (hstack)
        Např. A = [1, 2]         
              B = [3, 4]
              torch.vsack((A, B)) = tensor([[1],
                                            [2],
                                            [3],
                                            [4]])
              torch.hsack((A, B)) = tensor([[1], [2], [3], [4]])
    Squeeze 
        Odstraní všechny dimenze o velikosti 1 z tensoru (odstraní hranaté zavorky)
        Udělá z matrixu [1, 9] -> [9]
                        [1, 9, 3, 1, 1] -> [9, 3]
    Unsqueeze
        Přidá dimenzi o velikosti 1 do tensoru
    Permute 
        Vrátí view vstupní hodnoty se specificky pěmněněným pořadím dimenzí,
    
Tvar / Shape
    Tvar určuje jak velký je list a kolik listů je v listu.     
    Prozor záleží na typu listu.
    List = [1, 2, 3]        Shape: (3)          1D, List        List ==     Array ==        Vektor
    List = [[1, 2, 3],      Shape: (2, 3)       2D, Matrix      Python      Programování    Matematika
            [1, 2, 3]]
    List = [[[1, 2, 3],    Shape: (3, 2, 3) = tři listy po dvou listech s třemi hodnotami
                [1, 2, 3]],                     3D, Array
            [[1, 2, 3],
                [1, 2, 3]],
            [[1, 2, 3],
                [1, 2, 3]]]

torch.tensor(X)         X, Y = zvolená hodnota
    Vytvoří tenzor na základě X.
    Pro každý typ tenzoru je nutné zdat X jinak :

    Scalar -> torch.tensor(X) 
    Vector -> torch.tensor([X, Y]) 
    V případě že se jedná o skalar je potřeba použít metodu .item()

torch.rand(X)           X = zvolená hodnota
    Vytvoří tenzor s tvarem X.
    Tento tensor v sobě budemít náhodné hodnoty.

torch.zeros(X)          X = zvolená hodnota
    Vytvoří tenzor o velikosti X pouze s nulami.

torch.arange(X)         X = zvolená hodnota
    Vytvoří řadu čísel. 
    Lze aplikovat tůzné parametry -> torch.arange(start=0, end=1000, step=77)

torch.matmul(tensor1, tensor2) = torch.mm(tensor1, tensor2)  # je to stejné
    Vynásobí mezi sebou zadané tenzory (v tomto případě tensor1 a tensor2)

model.state_dict()              model = model / neuronová síť
    Vypíše jednotlivé parametry modelu i s jejich názvy.
Transpose
    Zmnění v metrixu řádky na sloupce -> X -> Y a Y -> X
    Volání: .T

Agregace
    Agregace =  hledání: sumi, min, max, mean...
    mean = průměr
    argmin(tensor) - najde idenx minimalni hodnoty v tensoru
    argmax(tensor) - najde idex maximalni hodnoty v tesnoru

Převá dění tenzoru z a do numpy
    Pro převedení numpy array na pytorch tenzor se používa metoda torch.from_numpy()
    Metoda from_numpy převede původní array tedy => pokud array upravíme sejně dostaneme puvodni verzi (ta kterou jsem vygenerovali)
    To stejné platí i pro převrácenou metodu torch.Tensor.numpy()
    Pro převrácen proces se používá torch.Tensor.numpy()
    Numpy ma defaultně formát float64 -> Long
    PyTorch ma defaultně formát float32 -> None

Reprodukce 
    Cílem reprodukce je vytvořit náhodná data která se ale pokaždé vyvoří stejná pokud jsou stejné i podmínky
    Pro reprodukci je dobré si vyvořit konstantu RANDOM_SEED = X (X = nahodné číslo, často 42)
    Poté se zavolá funkce torch.manual_seed(RANDOM_SEED) 
    Atímto se nastavý že pokud se vygenerují tenzory s náhodnýmy čísli o stejném tvaru tak budou mít identické hodnoty
    Tedy: 
        torch.manual_seed(RANDOM_SEED)
        random_tensor_C = torch.randn(3, 4)
        torch.manual_seed(RANDOM_SEED)
        random_tensor_D = torch.randn(3, 4) 
    v tomto případě bue platit že : random_tensor_C = random_tensor_D

Spouštění PyTorch programů na GPU
    Velmi se vyplatí spouště programi na GPU prože matematické operace budou probíhat výrazně rychleji.
    POZOR -> Numpy fnguje pouze na CPU 

Přesouvání dat mezi CPU a GPU
    Pro převedení na GPU -> tensor.to(device)      # device = "cuda"
    Pro převedení na CPU -> tensor.cpu()

Workflow / Pracovní popis
    Obsah:
        1. Příprava dat 
        2. Stavba modelu
        3. Učení modelu na připravených datech datech
        4. Dělání předpovědí a hodnocení modelu
        5. Ukládání a vkládání modelu
        6. Kombinování všeho předešlého dohromady
    from torch import nn -> vše co PyTorch nabýzí pro neuronové sítě.

                                        MODEL  =  NEURONOVÁ SÍŤ
tahák -> https://pytorch.org/tutorials/beginner/ptcheat.html
Forward metoda / def forward()
    Jedná se o součást většiny class, funkcí a modelů.
    Forward je ta část která realě dělá výpočet výsledku dané claass nebo modelu.
    Všechny podkategorie nn.Module (nn.Parametr...) požadují po svém připsání přepsání forward metody.
    přiklad forward metody :
        def foorward(self, x: torch.Tensor) -> torch.Tensor:    vysvětlení:
        říká že x má format torch tensor a ze funkce vrátí torch tensor

from torch import nn
    Obsahuje všechny před nastavené funkce a modele potřebné ke konstrukci neuronové sítě 
    které může pytoch nabýdnout.

torch.nn.Parametr
    Které parametry by se síť mněla naučit a vyskoušet
    => hodnoty které za nás síť sama dosadí a by dosáhla požadovaného výsledku.

torch.nn.module
    Základni class pro všechny moduly (neuronové sítě)
    Pokud zní necháváme dědit date (subclass) je nutné přepsat forward metodu

nn.Linear()
    Vytvoří vrstvu pro linearni model.
    K tomu potřebuje mit nadefinovány parametry 
    nn.Linear(in_features=1,    # kolik hodnotot dostane vyrstva -> 1 X_train
            out_features=1)   # kolik hodnotot vyplinve vyrstva -> 1 y_train
    Využívá pro vypočet rovnici -> Y = weight * x + bias

torch.optim
    Místo kde jsou uloženy optimazeri daného modulu.
    (např. pomáhá s gradial desent)

with torch.inference_mode():    inference = predikce
    Určí že výsledky této "funkce".
    Odpojí všechny kontroly a zbýraní dat které se dělá při učení sítě na jejích výsledcích.
    Výsledk? -> rychlejší získání výsledků (predikcí)
Data
    Data můžou bít skoro vše např.: video, fotka, text...
    Deeplearnig se skládá z vou hlavních kroků:
        Převedení dat na čísla.
        Vytvoření modelu aby mezi těmito čísli našel spojitosti
    Jeden z nejdůležitějších konceptů v práci s daty je(generalizace):
        Rozděleni dat na trénigové ,ohodnocené a testové data:
        Trénigové data / training st/split 60-80% všech dat
            Data na kterýchs e model učí.
        Ohodnocená data / validation set 10-20% všech dat    (není nutné mít tyto data ale je to občas lepší.)
            Zde hodnotíme jak dobře model data zpracoval.
        Testová data / test set/split    10-20% všech dat
            Z těcto zpracovaných dat nám model dá finální výsledek.     Předvede svou generalizaci
    Generalizace
        Schopnost modelu dobře spracovat data která jesše neviděl

Linear regression
    Vzorec: Y = f(X,B) + e

Parametr
    Je součást rovnice/vzorce se se modelu učí.
    Např.: weight, bias....

Cost funkce = Loss funkce = criterion
    Funkce ketá porovnává výsledek neouronové síť s požadovaným / správným  výsledkem.
    Toho dosahuje odečtem požadovaných hodnot od hodnout výsledných sítě a následným umocněním výsledku na 2
    => určuje jak špetné jsou výsledky neuronové sítě (čím menší číslo tím lepší)

Optimazer
    Optimazer využí vá loss funkce a na jejím základě upravuje model tak aby co nejefektivnějí
    snížil hodnotu loss funkce.
    Jako optimizeri se používa spousta algoritmů 
    V PyToch je najdeme pod torch.optim -> https://pytorch.org/docs/stable/optim.html

Gradiant desent
    Jedná se o algoritmus za cílem najít lokální minimum funkce.
    Tedy hledá nejnižší možný bod na grafu funkce.
    Tohto leze dosáhnout pomocí takzvané cost funkce která určuje chybnost výpočtu neruronové sítě
    Zároveň tento algoritmus využívá derivce ((takové té tenčny na grafu ;) )

Back propagation
    Jedná se o algoritmus který pomáhá učit neuronovou síť a upřesňuje její výsledky.
    Tohoto dosahuje pomocí určování duležitosti -> míra vlivu na cost funkci jednotlyvých parametrů.

lr=X        X -> zvolená hodnota (defaultnš je 0.1)
    Jedná se o learnig rate.
    Určuje jak moc se zmnění parametry během jednoho opakování

prams=model.Parametr()                  model = neuronová síť
    Určí optimizeru které parametry má upravit

Hyperparamet
    Parametr nastavený člověkem.

training loop
    Jak funguje:
    0. Proskenování dat
    1. Forward pass (prohnání dat skrz metodu forward()) - také nazívané jako forwad propagation
        za cílem vytvořit nějákou před předpovědí
    2. Zpočítání loss funkce - (srovnání výsledku modelu s správnýmy výsledky)
    3. Optimazer zero grad  nastavý hodnotu optimazeru na 0 aby se nezpomaloval výpočet (jak? nevím)
    4. Loss backkwards -> pohybuje se od výsledku aby zjistíl spád jednolivých parametrů
        => (spád -> míra významnosti pro výsledek)  - (Back propagation)
    5. Optimazer step - Využití optimizeru k upravě parametrů tak aby se snížila hodnota 
        z loss funkce. - (Gradiant desent)

epocha /epoch
    Je jedno opakování cyklu

model.train()       Model = neuronová sít
    Nastavý všechny parametry které potřebukí sklon aby potřebovaly sklon

Ukládání a nahrávání modelů v pytorch - https://pytorch.org/tutorials/beginner/saving_loading_models.html
    1. pomocí torch.save() - uložený předmnět v pytorch do formatů pickle od Pythonu
    2. torch.load() - umožní nahrát předmnět v pytoch
    3. torch.nn.Module.load_state_dict() - umožní uložit všechny parametry modelu
    Po uložení čistě parametru modelu je vhodné udělat nový model a nahrát parametry tam
    pth -> formát pro pythorch

Postup Ukládání + nahrávání (state_dict -> parametry)
    1. Je nutné importovat knihonu pathlib a následně zní pathlib
        import pathlib 
        from pathlib import Path
    2. Vytvoří se a pojmenuje místo kam se bude model ukládat (pomocí Path)
        MODEL_PATH = Path("models")
        MODEL_PATH.mkdir(parents=True, exist_ok=True)
    3. Pojmenování modelu
        MODEL_NAME = "___"
    4. Určí se místo uložení (pomocí cesty /path)
        MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME
    5. Samotné uložení
        torch.save(obj=model_0.state_dict(), f=MODEL_SAVE_PATH)
    6. Je vhodné parametry nahrát do nového modelu ale jinak :
        Loaded_model.load_state_dict(torch.load(f=MODEL_SAVE_PATH))

